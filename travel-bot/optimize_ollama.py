#!/usr/bin/env python3
"""
Ollama ì„±ëŠ¥ ìµœì í™” ë° íƒ€ì„ì•„ì›ƒ ë¬¸ì œ í•´ê²° ìŠ¤í¬ë¦½íŠ¸
"""

import subprocess
import requests
import time
import json
import os

def check_ollama_status():
    """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            return True, "Ollama ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."
        else:
            return False, f"Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}"
    except requests.exceptions.ConnectionError:
        return False, "Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return False, f"Ollama ì„œë²„ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}"

def get_ollama_models():
    """ì„¤ì¹˜ëœ Ollama ëª¨ë¸ ëª©ë¡ í™•ì¸"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            return [model["name"] for model in models]
        return []
    except:
        return []

def optimize_ollama_config():
    """Ollama ì„¤ì • ìµœì í™”"""
    print("=== Ollama ì„±ëŠ¥ ìµœì í™” ===")
    
    # 1. ëª¨ë¸ ì •ë³´ í™•ì¸
    models = get_ollama_models()
    print(f"ì„¤ì¹˜ëœ ëª¨ë¸: {models}")
    
    # 2. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
    print("\n=== ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸ ===")
    try:
        # CPU ì •ë³´
        cpu_info = subprocess.check_output("wmic cpu get name", shell=True).decode()
        print(f"CPU: {cpu_info.split('\\n')[1].strip()}")
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        memory_info = subprocess.check_output("wmic computersystem get TotalPhysicalMemory", shell=True).decode()
        memory_gb = int(memory_info.split('\\n')[1].strip()) / (1024**3)
        print(f"ì´ ë©”ëª¨ë¦¬: {memory_gb:.1f} GB")
        
        # ë””ìŠ¤í¬ ì •ë³´
        disk_info = subprocess.check_output("wmic logicaldisk get size,freespace,caption", shell=True).decode()
        print(f"ë””ìŠ¤í¬ ì •ë³´:\n{disk_info}")
        
    except Exception as e:
        print(f"ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # 3. Ollama ìµœì í™” ê¶Œì¥ì‚¬í•­
    print("\n=== Ollama ìµœì í™” ê¶Œì¥ì‚¬í•­ ===")
    print("1. ëª¨ë¸ í¬ê¸° ìµœì í™”:")
    print("   - gemma3:1b (í˜„ì¬ ì‚¬ìš© ì¤‘) - ì¢‹ì€ ì„ íƒ")
    print("   - ë” ë¹ ë¥¸ ì‘ë‹µì„ ì›í•œë‹¤ë©´: gemma3:1b-instruct")
    print("   - ë” ì •í™•í•œ ì‘ë‹µì„ ì›í•œë‹¤ë©´: llama3.2:3b")
    
    print("\n2. ì‹œìŠ¤í…œ ìµœì í™”:")
    print("   - ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ (ë©”ëª¨ë¦¬ í™•ë³´)")
    print("   - SSD ì‚¬ìš© ê¶Œì¥")
    print("   - ì¶©ë¶„í•œ ëƒ‰ê° (CPU ì˜¨ë„ ê´€ë¦¬)")
    
    print("\n3. Ollama ì„¤ì • ìµœì í™”:")
    print("   - num_predict: 300 (ì‘ë‹µ ê¸¸ì´ ì œí•œ)")
    print("   - temperature: 0.7 (ì ë‹¹í•œ ì°½ì˜ì„±)")
    print("   - top_k: 40 (ì‘ë‹µ í’ˆì§ˆê³¼ ì†ë„ ì¡°ì ˆ)")
    print("   - repeat_penalty: 1.1 (ë°˜ë³µ ë°©ì§€)")
    
    print("\n4. ë„¤íŠ¸ì›Œí¬ ìµœì í™”:")
    print("   - localhost ì—°ê²° ì‚¬ìš© (í˜„ì¬ ì„¤ì •)")
    print("   - ë°©í™”ë²½ ì„¤ì • í™•ì¸")
    print("   - í”„ë¡ì‹œ ì„¤ì • í™•ì¸")

def test_ollama_performance():
    """Ollama ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\n=== Ollama ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
    
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì„œìš¸ ì—¬í–‰ ì¶”ì²œí•´ì£¼ì„¸ìš”",
        "ì¼ë³¸ ë„ì¿„ 3ì¼ ì—¬í–‰ ê³„íš ì„¸ì›Œì£¼ì„¸ìš”"
    ]
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\ní…ŒìŠ¤íŠ¸ {i}: {prompt}")
        start_time = time.time()
        
        try:
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "gemma3:1b",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "num_predict": 300,
                        "top_k": 40,
                        "repeat_penalty": 1.1
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                elapsed_time = time.time() - start_time
                result = response.json()
                response_text = result.get("response", "")
                print(f"ì‘ë‹µ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                print(f"ì‘ë‹µ ê¸¸ì´: {len(response_text)} ë¬¸ì")
                print(f"ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:100]}...")
                
                if elapsed_time > 20:
                    print("âš ï¸  ì‘ë‹µ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤!")
                elif elapsed_time > 10:
                    print("âš ï¸  ì‘ë‹µ ì‹œê°„ì´ ë‹¤ì†Œ ê¹ë‹ˆë‹¤.")
                else:
                    print("âœ… ì‘ë‹µ ì‹œê°„ì´ ì ì ˆí•©ë‹ˆë‹¤.")
            else:
                print(f"âŒ ì˜¤ë¥˜: {response.status_code}")
                
        except requests.exceptions.Timeout:
            print("âŒ íƒ€ì„ì•„ì›ƒ ë°œìƒ")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")

def restart_ollama():
    """Ollama ì¬ì‹œì‘"""
    print("\n=== Ollama ì¬ì‹œì‘ ===")
    
    try:
        # Windowsì—ì„œ Ollama í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
        subprocess.run("taskkill /f /im ollama.exe", shell=True, capture_output=True)
        print("Ollama í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œë¨")
        
        time.sleep(2)
        
        # Ollama ì¬ì‹œì‘
        subprocess.Popen("ollama serve", shell=True)
        print("Ollama ì„œë²„ ì¬ì‹œì‘ ì¤‘...")
        
        # ì„œë²„ ì‹œì‘ ëŒ€ê¸°
        for i in range(10):
            time.sleep(1)
            if check_ollama_status()[0]:
                print("âœ… Ollama ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            print(f"ì„œë²„ ì‹œì‘ ëŒ€ê¸° ì¤‘... ({i+1}/10)")
        
        print("âŒ Ollama ì„œë²„ ì‹œì‘ ì‹¤íŒ¨")
        return False
        
    except Exception as e:
        print(f"âŒ Ollama ì¬ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Ollama ì„±ëŠ¥ ìµœì í™” ë„êµ¬")
    print("=" * 50)
    
    # 1. Ollama ìƒíƒœ í™•ì¸
    is_running, status_msg = check_ollama_status()
    print(f"Ollama ìƒíƒœ: {status_msg}")
    
    if not is_running:
        print("\nOllamaê°€ ì‹¤í–‰ë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ Ollamaë¥¼ ì‹œì‘í•˜ì„¸ìš”:")
        print("ollama serve")
        return
    
    # 2. ì„±ëŠ¥ ìµœì í™” ì •ë³´
    optimize_ollama_config()
    
    # 3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    test_ollama_performance()
    
    # 4. ì¬ì‹œì‘ ì˜µì…˜
    print("\n" + "=" * 50)
    restart_choice = input("\nOllamaë¥¼ ì¬ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
    
    if restart_choice == 'y':
        if restart_ollama():
            print("\nì¬ì‹œì‘ í›„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        else:
            print("\nì¬ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ollama serveë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    print("\nìµœì í™” ì™„ë£Œ!")

if __name__ == "__main__":
    main() 