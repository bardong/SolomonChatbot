"""
Colabì„ í†µí•œ í•œêµ­ì–´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ

1. Colabì—ì„œ ì‹¤í–‰í•  ì½”ë“œ:
"""

COLAB_CODE = '''
# Colabì—ì„œ ì‹¤í–‰í•  ì½”ë“œ
!pip install transformers torch accelerate bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
model_name = "beomi/Llama-3-Open-Ko-8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,
    device_map="auto",
    torch_dtype=torch.float16
)

# 2. í…ŒìŠ¤íŠ¸
test_prompt = "ì„œìš¸ 3ì¼ ì—¬í–‰ ì¶”ì²œí•´ì¤˜"
inputs = tokenizer(test_prompt, return_tensors="pt")
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_length=200,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"ì§ˆë¬¸: {test_prompt}")
print(f"ë‹µë³€: {response}")

# 3. ëª¨ë¸ ì €ì¥
model.save_pretrained("./llama3-korean-local")
tokenizer.save_pretrained("./llama3-korean-local")

# 4. ì••ì¶• íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ
!zip -r llama3-korean-local.zip ./llama3-korean-local/
from google.colab import files
files.download('llama3-korean-local.zip')
'''

def show_colab_guide():
    """Colab ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ ì¶œë ¥"""
    print("ğŸš€ Colabì„ í†µí•œ í•œêµ­ì–´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ")
    print("=" * 60)
    print()
    print("1ï¸âƒ£ Colabì—ì„œ ì‹¤í–‰í•  ì½”ë“œ:")
    print("-" * 40)
    print(COLAB_CODE)
    print()
    print("2ï¸âƒ£ ë¡œì»¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•:")
    print("-" * 40)
    print("""
# 1. Colabì—ì„œ ë‹¤ìš´ë¡œë“œí•œ zip íŒŒì¼ì„ ë¡œì»¬ë¡œ ë³µì‚¬
# 2. ì••ì¶• í•´ì œ: llama3-korean-local.zip â†’ llama3-korean-local í´ë”
# 3. ë¡œì»¬ì—ì„œ ëª¨ë¸ ë¡œë“œ:

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ë¡œì»¬ ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    "./llama3-korean-local",
    load_in_8bit=True,
    device_map="auto",
    torch_dtype=torch.float16
)
tokenizer = AutoTokenizer.from_pretrained("./llama3-korean-local")

# í…ŒìŠ¤íŠ¸
prompt = "ì„œìš¸ 3ì¼ ì—¬í–‰ ì¶”ì²œí•´ì¤˜"
inputs = tokenizer(prompt, return_tensors="pt")
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_length=200,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"ë‹µë³€: {response}")
""")
    print()
    print("3ï¸âƒ£ Ollamaì— ë“±ë¡í•˜ëŠ” ë°©ë²•:")
    print("-" * 40)
    print("""
# Modelfile ìƒì„± (backend í´ë”ì—)
FROM ./llama3-korean-local
PARAMETER temperature 0.7
PARAMETER top_p 0.9

# Ollamaì— ë“±ë¡
ollama create llama3-korean -f Modelfile

# ì‚¬ìš©
ollama run llama3-korean
""")

if __name__ == "__main__":
    show_colab_guide() 