import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def merge_lora_weights(
    base_model_name="beomi/Llama-3-Open-Ko-8B",
    lora_model_path="./korean_travel_lora",
    output_dir="./korean_travel_merged"
):
    """LoRA ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë³¸ ëª¨ë¸ê³¼ ë³‘í•©"""
    
    logger.info("LoRA ê°€ì¤‘ì¹˜ ë³‘í•© ì‹œì‘...")
    
    try:
        # 1. ê¸°ë³¸ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        logger.info(f"ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì¤‘: {base_model_name}")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        base_tokenizer = AutoTokenizer.from_pretrained(
            base_model_name,
            trust_remote_code=True,
            use_fast=False
        )
        
        # 2. LoRA ëª¨ë¸ ë¡œë“œ
        logger.info(f"LoRA ëª¨ë¸ ë¡œë”© ì¤‘: {lora_model_path}")
        model = PeftModel.from_pretrained(base_model, lora_model_path)
        
        # 3. LoRA ê°€ì¤‘ì¹˜ ë³‘í•©
        logger.info("LoRA ê°€ì¤‘ì¹˜ ë³‘í•© ì¤‘...")
        merged_model = model.merge_and_unload()
        
        # 4. ë³‘í•©ëœ ëª¨ë¸ ì €ì¥
        logger.info(f"ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì¤‘: {output_dir}")
        merged_model.save_pretrained(output_dir)
        base_tokenizer.save_pretrained(output_dir)
        
        logger.info("âœ… LoRA ê°€ì¤‘ì¹˜ ë³‘í•© ì™„ë£Œ!")
        
        return merged_model, base_tokenizer
        
    except Exception as e:
        logger.error(f"ë³‘í•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def test_merged_model(model, tokenizer, test_questions):
    """ë³‘í•©ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    logger.info("ë³‘í•©ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    model.eval()
    
    for question in test_questions:
        prompt = f"### ì§ˆë¬¸:\n{question}\n\n### ë‹µë³€:\n"
        
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.replace(prompt, "").strip()
        
        print(f"\nì§ˆë¬¸: {question}")
        print(f"ë‹µë³€: {response}")
        print("-" * 50)

def create_ollama_modelfile(output_dir="./korean_travel_merged"):
    """Ollamaìš© Modelfile ìƒì„±"""
    modelfile_content = f"""FROM {output_dir}

# í•œêµ­ì–´ ì—¬í–‰ ì±—ë´‡ ëª¨ë¸
# beomi/Llama-3-Open-Ko-8B ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ ì—¬í–‰ ì •ë³´ì— íŠ¹í™”ëœ ëª¨ë¸

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM """ë‹¹ì‹ ì€ í•œêµ­ì–´ ì—¬í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œêµ­ì˜ ë‹¤ì–‘í•œ ë„ì‹œì™€ ì§€ì—­ì— ëŒ€í•œ ì—¬í–‰ ì •ë³´, ë§›ì§‘ ì¶”ì²œ, ê´€ê´‘ì§€ ì•ˆë‚´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í•­ìƒ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""
"""
    
    with open("Modelfile_korean_travel", "w", encoding="utf-8") as f:
        f.write(modelfile_content)
    
    logger.info("âœ… Ollamaìš© Modelfile ìƒì„± ì™„ë£Œ: Modelfile_korean_travel")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ì„œìš¸ 2ì¼ ì—¬í–‰ ì¶”ì²œí•´ì¤˜",
        "ë¶€ì‚° ë§›ì§‘ ì•Œë ¤ì¤˜",
        "í•œêµ­ì–´ë¡œ ì¸ì‚¬í•´ì¤˜",
        "ì œì£¼ë„ ìì—° ê´€ê´‘ ì¶”ì²œí•´ì¤˜"
    ]
    
    try:
        # 1. LoRA ê°€ì¤‘ì¹˜ ë³‘í•©
        merged_model, tokenizer = merge_lora_weights()
        
        # 2. ë³‘í•©ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        test_merged_model(merged_model, tokenizer, test_questions)
        
        # 3. Ollamaìš© Modelfile ìƒì„±
        create_ollama_modelfile()
        
        print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("ğŸ“ ë³‘í•©ëœ ëª¨ë¸: ./korean_travel_merged")
        print("ğŸ“ Ollama Modelfile: Modelfile_korean_travel")
        print("\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ Ollamaì— ëª¨ë¸ì„ ë“±ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("ollama create korean-travel -f Modelfile_korean_travel")
        
    except Exception as e:
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    main() 