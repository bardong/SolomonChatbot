"""
Colabì„ í†µí•œ í•œêµ­ ì—¬í–‰ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° íŒŒì¸íŠœë‹ ê°€ì´ë“œ

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Google Colabì—ì„œ í•œêµ­ì–´ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ ,
ë¡œì»¬ì—ì„œ íŒŒì¸íŠœë‹í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.
"""

COLAB_DOWNLOAD_CODE = '''
# Colabì—ì„œ ì‹¤í–‰í•  ì½”ë“œ - ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
!pip install transformers torch accelerate bitsandbytes peft datasets

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import json

# 1. ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (llama3.2:1b ëŒ€ì‹  ì‚¬ìš©í•  í•œêµ­ì–´ ëª¨ë¸)
model_name = "beomi/Llama-3-Open-Ko-8B"
print(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_name}")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,
    device_map="auto",
    torch_dtype=torch.float16
)

# 2. ëª¨ë¸ í…ŒìŠ¤íŠ¸
test_prompt = "ì„œìš¸ 3ì¼ ì—¬í–‰ ì¶”ì²œí•´ì¤˜"
inputs = tokenizer(test_prompt, return_tensors="pt")
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_length=200,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"ì§ˆë¬¸: {test_prompt}")
print(f"ë‹µë³€: {response}")

# 3. ëª¨ë¸ ì €ì¥
print("ëª¨ë¸ ì €ì¥ ì¤‘...")
model.save_pretrained("./llama3-korean-base")
tokenizer.save_pretrained("./llama3-korean-base")

# 4. ì••ì¶• íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ
!zip -r llama3-korean-base.zip ./llama3-korean-base/
from google.colab import files
files.download('llama3-korean-base.zip')
print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ! llama3-korean-base.zip íŒŒì¼ì„ ë¡œì»¬ë¡œ ë³µì‚¬í•˜ì„¸ìš”.")
'''

COLAB_FINETUNE_CODE = '''
# Colabì—ì„œ ì‹¤í–‰í•  ì½”ë“œ - íŒŒì¸íŠœë‹ (ì„ íƒì‚¬í•­)
!pip install transformers torch accelerate bitsandbytes peft datasets

from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType
import torch
import json

# 1. í•œêµ­ ì—¬í–‰ ë°ì´í„°ì…‹ ìƒì„±
travel_data = [
    {"input": "ì„œìš¸ 3ì¼ ì—¬í–‰ ì¶”ì²œí•´ì¤˜", "output": "ì„œìš¸ 3ì¼ ì—¬í–‰ ì¼ì •ì„ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤!\\n\\n### Day 1 - ì „í†µê³¼ í˜„ëŒ€ì˜ ì¡°í™”\\n08:00 ì•„ì¹¨: ì¸ì²œê³µí•­ ë„ì°© ë° í˜¸í…” ì²´í¬ì¸\\n10:00 ê²½ë³µê¶ ê´€ëŒ (ì…ì¥ë£Œ 3,000ì›)\\n12:00 ì ì‹¬: ê´‘ì¥ì‹œì¥ ì§„ì£¼ì§‘ ìˆœëŒ€êµ­ë°¥ (8,000ì›)\\n14:00 ì°½ë•ê¶ê³¼ í›„ì› ê´€ëŒ (ì…ì¥ë£Œ 3,000ì›)\\n16:00 ë¶ì´Œí•œì˜¥ë§ˆì„ ì‚°ì±…\\n18:00 ì €ë…: ì„ì§€ë©´ì˜¥ í‰ì–‘ëƒ‰ë©´ (12,000ì›)\\n20:00 ë‚¨ì‚°íƒ€ì›Œ ì•¼ê²½ (ì…ì¥ë£Œ 16,000ì›)\\n\\n### Day 2 - ì‡¼í•‘ê³¼ ë¬¸í™”\\n09:00 ì•„ì¹¨: ëª…ë™ ì‡¼í•‘ê±°ë¦¬\\n12:00 ì ì‹¬: ëª…ë™êµì ì¹¼êµ­ìˆ˜ (9,000ì›)\\n14:00 í™ëŒ€ ê±°ë¦¬ ì‡¼í•‘ê³¼ ì¹´í˜íˆ¬ì–´\\n16:00 ì´íƒœì› ê±°ë¦¬ ê´€ê´‘\\n18:00 ì €ë…: í•œë‚¨ë™ ë§›ì§‘ íˆ¬ì–´\\n20:00 í•œê°•ê³µì› ì•¼ê²½ ì‚°ì±…\\n\\n### Day 3 - ìì—°ê³¼ íœ´ì‹\\n09:00 ì•„ì¹¨: í•œê°•ê³µì› ìì „ê±° íƒ€ê¸° (ëŒ€ì—¬ 3,000ì›/ì‹œê°„)\\n12:00 ì ì‹¬: ê°•ë‚¨ ìš°ë˜ì˜¥ í•œìš°êµ¬ì´ (25,000ì›)\\n14:00 ê°•ë‚¨ì—­ ì‡¼í•‘\\n16:00 ì½”ì—‘ìŠ¤ëª°ê³¼ ìŠ¤íƒ€í•„ë“œ\\n18:00 ì €ë…: ê°•ë‚¨ì—­ ë§›ì§‘\\n\\nêµí†µí¸: ì§€í•˜ì²  1ì¼ê¶Œ 4,500ì›, êµí†µì¹´ë“œ ì¶©ì „ ê¶Œì¥"},
    {"input": "ë¶€ì‚° ë§›ì§‘ ì¶”ì²œí•´ì¤˜", "output": "ë¶€ì‚°ì˜ ëŒ€í‘œ ë§›ì§‘ë“¤ì„ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤!\\n\\n### í•´ì‚°ë¬¼ ë§›ì§‘\\n1. ìê°ˆì¹˜ì‹œì¥ íšŸì§‘ - ì‹ ì„ í•œ íšŒì™€ í•´ì‚°ë¬¼ (25,000ì›)\\n2. í•´ìš´ëŒ€ í•´ë¬¼íƒ• - í•´ì‚°ë¬¼ ì „ë¬¸ (15,000ì›)\\n3. ë¶€ì‚° íšŒì„¼í„° - ë‹¤ì–‘í•œ í•´ì‚°ë¬¼ (20,000ì›)\\n4. ì†¡ë„ í•´ìˆ˜ìš•ì¥ ë§›ì§‘ - ë°”ë‹¤ ì „ë§ (18,000ì›)\\n5. ë¯¼ë½ìˆ˜ë³€í¬ì°¨ - ì•¼ê²½ê³¼ í•¨ê»˜ (12,000ì›)\\n\\n### ë¶€ì‚° íŠ¹ì‚°ë¬¼\\n1. ë¶€ì‚° ë¼ì§€êµ­ë°¥ - ë¶€ì‚°ì˜ ëŒ€í‘œ ìŒì‹ (8,000ì›)\\n2. ì„œë©´ ë°€ë©´ - ë¶€ì‚° íŠ¹ì‚°ë¬¼ (7,000ì›)\\n3. ê°ì²œ ë§›ì§‘ - ê°ì²œë¬¸í™”ë§ˆì„ ë§›ì§‘ (10,000ì›)\\n4. ë‚¨í¬ë™ ë§›ì§‘ - ë‚¨í¬ë™ BIFFê´‘ì¥ ê·¼ì²˜ (12,000ì›)\\n5. ë¶€ì‚°ì—­ ë§›ì§‘ - ë¶€ì‚°ì—­ ê·¼ì²˜ ë§›ì§‘ (9,000ì›)\\n\\n### íŠ¹ì§•\\n- ë¶€ì‚°ì˜ í˜„ì§€ ë§›ì§‘ë“¤ì…ë‹ˆë‹¤\\n- í•´ì‚°ë¬¼ê³¼ ë¶€ì‚° íŠ¹ì‚°ë¬¼ì„ ë§›ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤\\n- ê°ê° ë‹¤ë¥¸ ë¶„ì•¼ì˜ ìŒì‹ì„ ë§›ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤"}
]

# ë°ì´í„°ì…‹ ì €ì¥
with open('korean_travel_dataset.jsonl', 'w', encoding='utf-8') as f:
    for item in travel_data:
        f.write(json.dumps(item, ensure_ascii=False) + '\\n')

# 2. ëª¨ë¸ ë¡œë“œ
model_name = "beomi/Llama-3-Open-Ko-8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,
    device_map="auto",
    torch_dtype=torch.float16
)

# 3. LoRA ì„¤ì •
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj"]
)

model = get_peft_model(model, lora_config)

# 4. ë°ì´í„°ì…‹ ì¤€ë¹„
def prepare_data():
    data = []
    with open('korean_travel_dataset.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line.strip())
            prompt = f"ì§ˆë¬¸: {item['input']}\\në‹µë³€: {item['output']}"
            data.append({"text": prompt})
    return data

dataset = prepare_data()

# 5. í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./lora_korean_travel",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    warmup_steps=100,
    logging_steps=10,
    save_steps=100,
    evaluation_strategy="no",
    save_total_limit=2,
)

# 6. í•™ìŠµ ì‹¤í–‰
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=lambda data: {'input_ids': torch.stack([torch.tensor(item) for item in data])}
)

print("íŒŒì¸íŠœë‹ ì‹œì‘...")
trainer.train()

# 7. ëª¨ë¸ ì €ì¥
model.save_pretrained("./lora_korean_travel_final")
tokenizer.save_pretrained("./lora_korean_travel_final")

# 8. ì••ì¶• ë° ë‹¤ìš´ë¡œë“œ
!zip -r lora_korean_travel_final.zip ./lora_korean_travel_final/
from google.colab import files
files.download('lora_korean_travel_final.zip')
print("íŒŒì¸íŠœë‹ ì™„ë£Œ! lora_korean_travel_final.zip íŒŒì¼ì„ ë¡œì»¬ë¡œ ë³µì‚¬í•˜ì„¸ìš”.")
'''

def show_enhanced_colab_guide():
    """í–¥ìƒëœ Colab ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ ì¶œë ¥"""
    print("ğŸš€ Colabì„ í†µí•œ í•œêµ­ ì—¬í–‰ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° íŒŒì¸íŠœë‹ ê°€ì´ë“œ")
    print("=" * 70)
    print()
    
    print("ğŸ“‹ ë‹¨ê³„ë³„ ì§„í–‰ ë°©ë²•:")
    print("1. Google Colabì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
    print("2. ë¡œì»¬ë¡œ íŒŒì¼ ë³µì‚¬")
    print("3. ë¡œì»¬ì—ì„œ íŒŒì¸íŠœë‹ ë˜ëŠ” Ollama ë“±ë¡")
    print()
    
    print("1ï¸âƒ£ Google Colabì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:")
    print("-" * 50)
    print("1. https://colab.research.google.com ì ‘ì†")
    print("2. ìƒˆ ë…¸íŠ¸ë¶ ìƒì„±")
    print("3. ì•„ë˜ ì½”ë“œë¥¼ ë³µì‚¬í•˜ì—¬ ì‹¤í–‰:")
    print()
    print("ğŸ“ ë‹¤ìš´ë¡œë“œ ì½”ë“œ:")
    print(COLAB_DOWNLOAD_CODE)
    print()
    
    print("2ï¸âƒ£ íŒŒì¸íŠœë‹ (ì„ íƒì‚¬í•­):")
    print("-" * 50)
    print("ë‹¤ìš´ë¡œë“œ í›„ ì¶”ê°€ë¡œ íŒŒì¸íŠœë‹ì„ ì›í•œë‹¤ë©´:")
    print()
    print("ğŸ“ íŒŒì¸íŠœë‹ ì½”ë“œ:")
    print(COLAB_FINETUNE_CODE)
    print()
    
    print("3ï¸âƒ£ ë¡œì»¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•:")
    print("-" * 50)
    print("""
# 1. Colabì—ì„œ ë‹¤ìš´ë¡œë“œí•œ zip íŒŒì¼ì„ ë¡œì»¬ë¡œ ë³µì‚¬
# 2. ì••ì¶• í•´ì œ: llama3-korean-base.zip â†’ llama3-korean-base í´ë”
# 3. ë¡œì»¬ì—ì„œ ëª¨ë¸ ë¡œë“œ:

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ë¡œì»¬ ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    "./llama3-korean-base",
    load_in_8bit=True,
    device_map="auto",
    torch_dtype=torch.float16
)
tokenizer = AutoTokenizer.from_pretrained("./llama3-korean-base")

# í…ŒìŠ¤íŠ¸
prompt = "ì„œìš¸ 3ì¼ ì—¬í–‰ ì¶”ì²œí•´ì¤˜"
inputs = tokenizer(prompt, return_tensors="pt")
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_length=200,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"ë‹µë³€: {response}")
""")
    print()
    
    print("4ï¸âƒ£ Ollamaì— ë“±ë¡í•˜ëŠ” ë°©ë²•:")
    print("-" * 50)
    print("""
# Modelfile ìƒì„± (backend í´ë”ì—)
FROM ./llama3-korean-base
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER max_tokens 500

# Ollamaì— ë“±ë¡
ollama create llama3-korean-travel -f Modelfile

# ì‚¬ìš©
ollama run llama3-korean-travel
""")
    print()
    
    print("5ï¸âƒ£ ë°±ì—”ë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•:")
    print("-" * 50)
    print("""
# app.pyì—ì„œ ëª¨ë¸ëª… ë³€ê²½
OLLAMA_MODEL = "llama3-korean-travel"  # ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì •

# ì„œë²„ ì¬ì‹œì‘
python app.py
""")
    print()
    
    print("âš ï¸  ì£¼ì˜ì‚¬í•­:")
    print("- Colabì€ ë¬´ë£Œ ë²„ì „ì—ì„œ ì„¸ì…˜ ì‹œê°„ì´ ì œí•œë©ë‹ˆë‹¤")
    print("- ëŒ€ìš©ëŸ‰ ëª¨ë¸ ë‹¤ìš´ë¡œë“œëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    print("- GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 8bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
    print()
    
    print("ğŸ¯ ê¶Œì¥ì‚¬í•­:")
    print("- ë¨¼ì € ë‹¤ìš´ë¡œë“œë§Œ ì§„í–‰í•˜ê³ , ì„±ê³µí•˜ë©´ íŒŒì¸íŠœë‹ì„ ì‹œë„í•˜ì„¸ìš”")
    print("- ë‹¤ìš´ë¡œë“œ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”")
    print("- ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸ í›„ Ollamaì— ë“±ë¡í•˜ì„¸ìš”")

if __name__ == "__main__":
    show_enhanced_colab_guide() 